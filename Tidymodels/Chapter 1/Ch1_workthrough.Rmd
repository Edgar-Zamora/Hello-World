---
title: "Tidymodels"
author: "Edgar Zamora"
date: "6/15/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
#Loading Packages
library(tidyverse)
library(tidymodels)
library(rstanarm)
library(rstanarm)
library(janitor)
library(broom)
library(randomForest)

#Importing Data
cars2018 <- readRDS("~/Desktop/GitHub/myrepo/Tidymodels/data/c1_car_vars.rds") %>% 
  clean_names()

knitr::opts_chunk$set(echo = FALSE)
```

# MPG Distribuation

Per the density plot below, one can see that a large portion of the cars have a MPG between 20 and 25. Also we can say that the MPG data is postively skewed in that the mean is greater in value than the median which leads to assume that there might a lot of cars that poor MPG like larger trucks or SUVs.

```{r mpg_density}
ggplot(cars2018, aes(mpg)) +
  geom_density(size = 1.5) +
  labs(
    x = "Fuel efficiency (mpg)",
    y = "Number of Cars",
    title = "MPG Distribution"
  ) +
  scale_x_continuous(breaks = seq(10, 60, by = 5)) +
  theme(
    panel.background = element_blank(),
    axis.ticks = element_blank()
  )
```

# Linear Modeling

Creating a linear model is quite easy. R offers the abilty to use the `lm()` function. The arguments that are needed is the dependent variable (left of tilda) and its features/independent variables (right of tilda). Finally the data is also feed through the data argument. 

```{r linear_model}
fit_all <- lm(mpg ~ ., data = cars2018)

fit_all %>%
  tidy()
```


# Getting started with tidymodels

```{r training_model, eval=FALSE, include=FALSE}
## a linear regresion model specficiation
#specifying the model without any particular reference to model. More of a framework
lm_mod <- linear_reg() %>% 
  set_engine("lm")

#Fitting the model to data
lm_fit <- lm_mod %>% 
  fit(log(mpg) ~ .,
      data = cars2018)



## a random forest model specification
rf_mod <- rand_forest() %>% 
  set_mode("regression") %>% 
  set_engine("randomForest")

fit_rf <- rf_mod %>% 
  fit(log(mpg) ~ .,
      data = cars2018)

```


The `yardstick` package is used to evaulate how well the model is performing. Functions in this package offer metrics to measure how well models are doing.


# Training and testing data

It is good practice to seperate some the original data into a training and testing data as to get a better estimate of how the model will perform on new data. Using the `rsample()` package will help to acheive the splitting of this data.

More often than not data is split 80/20 where 80% goes into training while 20% goest into testing. The reason for doing such a split is to balance charactertics in the data.

Holding out testing data allows one to assess if a model is being overfit. 

Creating training/testing splits reduces overfitting. You get better estimates when you evaulate your model(s) on data that it (the model) has not seen/trained on. 

```{r train_test}
library(rsample)

set.seed(1234)

car_split <- cars2018 %>% 
  initial_split(prop = .80,
                strata = transmission)

#<[train]/[testing]/[total]>

car_training <- training(car_split)
car_testing <- testing(car_split)

```


# Train models with tidymodels

```{r training_models}
#Linear Model
#Build a linear regression model specification
lm_mod <- linear_reg() %>% 
  set_engine("lm")

#Train a linear regression model
fit_lm <- lm_mod %>% 
  fit(log(mpg) ~ .,
      data = car_training)

#Print the model object
fit_lm %>% 
  tidy()


#Random Forest
#Build a random forest model specificion
rf_mod <- rand_forest() %>% 
  set_engine("randomForest") %>% 
  set_mode("regression")

#Train a random forest model
fit_rf <- rf_mod %>% 
  fit(log(mpg) ~ .,
      data = car_training)

#Print the model object
fit_rf
```


# Evaluate model performance

There are several things to consider, including both what *metrics* and what *data* to use, when determine how our models did.

**Regression models**: Focus on evaluating use the **root mean squared error** metric. Measured in the same units as the dependent variable. A lower root mean squarted error indicates a better fit to the data. The `yardstick` package offer convenient functions.


```{r evaluation}
#Creating new columns
results <- car_training %>% 
  mutate(mpg = log(mpg)) %>% 
  bind_cols(predict(fit_lm, car_training) %>% #model predictions for each of the models
              rename(.pred_lm = .pred)) %>% 
  bind_cols(predict(fit_rf, car_training) %>% 
              rename(.pred_rf = .pred))


#Evaluate the performance
metrics(results, truth = mpg, estimate = .pred_lm)
metrics(results, truth = mpg, estimate = .pred_rf)

```

Based on the above output, we can see that the random forest model has a lower **root mean squared error** lead us to prematurally say that the random forest model is a better fit for the data. However, this is premature becuase the fit and subsequent output were done on the *training* data which was used to build the model. Therefore the model is said to be optimistic in its estiamtion so we need to use the *testing* data now.


# Using the testing data

We have just trained our models on the entire training set once and then proceded to evaluate the testing data. Though fine it does not offer much in terms of rebustness. Instead a better approach is to to use the method of **resampling**. The idea of resampling is to create simulated data sets that can be used to estimate performance of your model when you want to compare models. The reason for this type of approach is that using the entire training set provides an overly opitmistic result while using your testing data more than twice renders the data meaningless. 

## Bootstrap

Bootstrap resampling means drawing with replacement (there is a possiblity of seeing the same datapoint twice in a datset) from our original datset and then fitting on that dataset. 

Essentially you draw randomly pull from the training set until you reach the same sample size as your training set, which in this case was 917. Again there is a probably of drawing the same observation mulitple times. After creating that new dataset the data is split into a training and testing dataset and fit with the model and evaluated. This process of sampling, fitting, and evaulating is repeated multiple times. After done a number of times an average of the perfromance metrics is taken.


```{r bootstraping, include=FALSE}
car_boot <- bootstraps(car_training)

#fitting a model to each resample and computing performance metrics for each
lm_results <- lm_mod %>% 
  fit_resamples(
    log(mpg) ~.,
    car_boot,
    control = control_resamples(save_pred = TRUE)
  )


rf_results <- rf_mod %>% 
  fit_resamples(
    log(mpg) ~.,
    car_boot,
    control = control_resamples(save_pred = TRUE)
  )


results2 <- bind_rows(lm_results %>% 
                        collect_predictions() %>% 
                        mutate(model = "lm"),
                      rf_results %>% 
                        collect_predictions() %>% 
                        mutate(model = "rf"))
```


Visualizing both models

```{r model_visualization}
results2 %>% 
  ggplot(aes(`log(mpg)`, .pred)) +
  geom_abline(lty = 2, color = "grey50") +
  geom_point(aes(color = id), size = 1.5, alpha = 0.3, show.legend = FALSE) +
  geom_smooth(method = "lm") +
  facet_wrap(~model)

```




